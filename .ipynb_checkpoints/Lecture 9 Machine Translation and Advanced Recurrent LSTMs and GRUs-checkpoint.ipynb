{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本节内容：\n",
    "1. 传统Machine Translation模型\n",
    "2. GRUs\n",
    "3. LSTM\n",
    "\n",
    "同样只上重点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current statistical machine translation systems\n",
    "source language f, target language e, Probabilistic formulation(using Bayers rule):\n",
    "<center>$\\hat{e}\\ =\\ argmax_e p(e|f)\\ =\\ argmax_e p(f|e)p(e)$</center>\n",
    "Translation model p(f|e) trained on parallel corpus<br>\n",
    "Language model p(e) trained on English onply corpus,就是上一节介绍的模型，起选择和润滑的作用<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![machine_translation_system](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/machine_translation_systems.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional MT\n",
    "#### Step1 for training translation model: Alignment\n",
    "Goal: know which word or pahrases in source language would translate to what words or phrases in target language?<br>\n",
    "不同语言之间进行翻译的时候，单词之间的关系可能不是一一对应的。事实上这种对应关系非常复杂，他可能是一对一，一对多，甚至是多对一的，而有的单词甚至是在其他语言中不存在对应关系的（像日语中的ます）（想想文原文，死的心都有了）。如果再考虑上短语的话，那么复杂程度就更甚了。<br>\n",
    "The consider reordering of translated pahrases\n",
    "#### After many steps\n",
    "Each phrase in source language has many possible translations resulting in large search space.<br>\n",
    "每个单词或组合可能对应多个翻译结果，这导致，一个简单的句子，也存在多个候选的翻译集，而这里还木考虑语序的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![translation_options](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/translation_options.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end to end model的概念\n",
    "一个模型由多个独立的模型组合而成，像上面提到的traditional MT，他由多块model组合而成，每个model独立完成训练。而end to end model是指把多个模块连接在一起，一次性训练所有的模型。而这也是deep learningmodel的一个优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT with RNNs - simplest Model\n",
    "Encoder: $h_t\\ =\\ \\phi(h_{t-1}, x_t)\\ =\\ f(W^{hh}h_{t-1}+W^{hx}x_t)$<br>\n",
    "Decoder: $h_t\\ = \\phi(h_{t-1})\\ =\\ f(W^{hh}h_{t-1})$<br>(为什么这是decoder的？)\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ y_t\\ =\\ softmax(W^Sh_t)$<br>\n",
    "Minimize cross entorpy error for all target words conditioned on source words:\n",
    "<center>$max_{\\theta}\\frac{1}{N}\\sum^N_{n=1}log\\ p_\\theta (y^{(n)}|x^{(n)})$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Translation Model Extensions\n",
    "1.Train different RNN weights for encoding and decoding <br>\n",
    "Notation: Each input $\\phi$ has its own linear transformation matrix. Simple: $h_t=\\phi (h_{t-1})=f(W^{hh}h_{t-1})$<br>\n",
    "2.Compute every hidden state in decoder from:<br>\n",
    "- Previous hidden state(standard)\n",
    "- Last hidden vector of encoder c = $h_t$\n",
    "- Previous predicted output word $y_{t-1}$可防止重复输出\n",
    "<center>$h_{D, t}\\ =\\ \\phi_D(h_{t-1},c,y_{t-1})$</center>\n",
    "\n",
    "3.Train stacked/deep RNNs with multiple layers<br>\n",
    "4.Potentially train bidirectional encoder<br>\n",
    "5.Train input sequence in reverse order for simpler optimization probem: instead of ABC ->XY, train with CBA -> XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRUs\n",
    "standard RNN computes hidden layer at next time step directly<br>\n",
    "GRU first computes an update gate(another layer) based on current ipput word vector and hidden state<br>\n",
    "值域[0,1]，用于调整对应对象的权重，这里是调整的是上一个h的影响程度\n",
    "<center>$z_t=\\sigma(W^{(z)}x_t+U^{(z)}h_{t-1})$</center>\n",
    "Compute reset gate similarly but with different weights<br>\n",
    "使能够消除，历史的影响。\n",
    "<center>$r_t=\\sigma(W^{(r)}x_t+U^{(r)}h_{t-1})$</center>\n",
    "new menory content:\n",
    "<center>$\\hat{h_t}=tanh(Wx_t+r_t\\circ Uh_{t-1})$</center>\n",
    "Final memory at time step combines current and previous time steps:\n",
    "<center>$h_t=z_t\\circ h_{t-1}+(1-z_t)\\circ \\hat{h_t}$</center>\n",
    "注意， 我们只是赋予网络具有某种能力，具体什么时候用，怎么用，网络自己会决策。\n",
    "Units with short-term dependencies often have reset gates very active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GRUs](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/GRUs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### long short term memories(LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/LSTM1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/LSTM2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
