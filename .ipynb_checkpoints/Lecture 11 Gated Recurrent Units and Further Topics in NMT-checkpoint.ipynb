{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gated recurrent unit的一个功能是减弱梯度消失的影响<br>\n",
    "在一般的rnn中，error must backpropagate through all the intermediate nodes。这也是导致梯度消失的主要原因，因此这里提出的一个想法就是：我们能不能不把所有的节点都遍历一遍，而是跳跃式的进行前进。这样当backPropagate的时候，就可以大大缩减传递的路径，从而削弱梯度消失的影响。而这也正是gated recurrent Unit的主要思想。\n",
    "<center>$f(h_{t-1},x_t)=u_t\\bigodot \\hat{h}_t+(1-u_t)\\bigodot h_{t-1}$</center>\n",
    "<center>$\\hat{h}_t=tanh(W[x_t]+Uh_{t-1}+b)$</center>\n",
    "<center>$u_t=\\sigma (W_u[x_t]+U_uh_{t-1}+b_u)$</center>\n",
    "一般的rnn中，$h_t$就是$\\hat{h}_t$。这里加入了$h_{t-1}$，当其权值为1时，表示$h_t$直接由$h_{t-1}$copy过来，从而传递到下一个时间点。这也就允许了节点的跳跃。可以直观的看到这是个线性函数，也就是说梯度为常数。这也就允许了error的跨步传播，也就是当前的error可以直接传给多步以前的节点，从而削减了梯度下降。\n",
    "更进一步的，也许我们会想要删掉一些节点的影响，为了实现这个功能，我们引入了reset gate\n",
    "<center>$\\hat{h}_t=tanh(W[x_t]+U(r_t\\bigodot h_{t-1}+b)$</center>\n",
    "<center>$r_t = \\sigma(W_r[x_t]+U_rh_{t-1}+b_r)$</center>\n",
    "注意区分这两个gates，update gate的引入实际上就是增加传播路径，但每个隐藏层的输出依旧受上一个时间点的隐藏层的输出的影响。（其实感觉这里的h应该被分成两个概念来解释，一个是隐藏层的输出h，一个是上下文因素c。这样解释起来会更加的方便。）。reset gate则正如他们名字，他用于重置历史记录（重置说得有点严重了，就是调整影响程度。）。综上我们可以说，reset gate是用来选择，哪些节点会影响后续节点，而update gate是用来选择，哪些节点，在backpropagate的过程中，会被重写。（一根筋的都梯度消失了）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与GRU相比LSTM把update gate分成两个不同的gates，即input gate和forget gate。input gate用来调整输入的影响，forget gate用来调整上一个时间点隐藏层输出的影响。另外去掉了reset gate，改为使用output gate，但是本质上是一样的（虽然reset gate比较直观，不过画图一看，估计就知道了，不过还是有区别的就是了）：\n",
    "<center>$h_t=o_t\\bigodot tanh(c_t)$</center>\n",
    "<center>$c_t=f_t\\bigodot c_{t-1}+i_t\\bigodot \\hat{c}_t$</center>\n",
    "<center>$\\hat{c}_t=tanh(W_c[x_t]+U_ch_{t-1}+b_c)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外几个训练RNN时应该注意的点：\n",
    "- Use an LSTM or GRU\n",
    "- Initialize recurrent matrices to be orthogonal\n",
    "- Initialize other matrices with a sensible(small) scale\n",
    "- Initialize forget gate bias to 1\n",
    "- Use adaptive learning rate algorithm: Adam,\n",
    "- Clip the norm of the gradient: 1-5 seems to be a reasonable threshold when used together with Adam or AdaDelta\n",
    "- Either only dropout vertically or lean how to do it right\n",
    "- Be patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这期的highlight，通过唇语和语音实现实时文本生成：基本模型用两个LSTM网络分别训练语音和图片（图片经cnn进行信息提取）。然后通过一个LSTM decoder进行text生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MT evaluation \n",
    "最常用的方法是BLEU(Bilingual Evaluation Understudy)，其核心思想是：\n",
    "N-gram precision(score is between 0~1)\n",
    "- what percent of machine n-grams can be found in the reference translation?\n",
    "- For each n-gram size, not allowed to match identical portion of reference translation more than once(two MT words airport are only correct if two reference words airport; can't cheat by typing out \"the the the...\"\n",
    "Brevity Penalty: Your translation is shorter then the reference translation \n",
    "- can' just type out single word \"the\"\n",
    "reference translation是指人为翻译。\n",
    "针对一个原文可以有多个译文的情况，IBM的建议是使用multiple reference Translation（建议4个）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impotant sampling <br>\n",
    "The word generation problem: dealing with a large output vocab<br>\n",
    "隐藏层输出输出连接的softmax，对应着整个单词表，其大小动则上万。<br>\n",
    "lots of ideas from the neural LM literature<br>\n",
    "- Hierarchical models: tree-structured vocabulary[Morin&Bengio, AISTATS'05]\n",
    "- Noise-contrastive estimation:binary classification[Mnih&Teh, ICML'12]\n",
    "以上方法not GPU friendly, GPU friendly的一种方法是\n",
    "Training: a subset of the vocabulary at a time\n",
    "Testing: smart on the set of possible translations\n",
    "we can partition the training data\n",
    "testing - select candidate words\n",
    "K most frequent words: unigram prob  一直使用\n",
    "Candidate target words: for each word in the source sentence look at the likely k' translation of it and throw those into our candidates for the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
