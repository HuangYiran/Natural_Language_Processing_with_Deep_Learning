{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn\n",
    "一般的rnn的限制：(一般指没加门算法):<br>\n",
    "rnn can't capture phrases without prefix context and often capture too much of last words in final vector.<br>\n",
    "\n",
    "#### What is convolution\n",
    "跳\n",
    "\n",
    "#### single layer cnn\n",
    "a simple variant using one convolutional layer and pooling.<br>\n",
    "句子是由各个单词词向量链接而成的向量。filter是向量，其大小为h个词向量，h为要看的单词数，h越大，每次获取的信息越多，但也更容易Overfitting。步长为1。因为之后会接pooling，所以句子的长度在这里影响并不大<br>\n",
    "to compute feature for CNN layer:<br>\n",
    "<center>$c_i=f(W^Tx_{i:i+h-1}+b)$</center>\n",
    "result a feature map:<br>（这里好像没有把padding算进去）\n",
    "<center>$c=[c_1,c_2,...,c_{n-h+1}]\\in R^{n-h+1}$</center>\n",
    "其长度由句子中的单词数决定。这也是引入pooling layer的一个重要原因\n",
    "\n",
    "#### pooling layer\n",
    "in particular: max-over-time pooling layer \n",
    "Idea: capture most important activation(maximun over time)\n",
    "我们希望的是，当filter窗口中的单词能够构成短语的时候，他对应的feature map是最大的。（这里也可以看出当h太大的时候，可能导致遗失重要信息）。换句话说，也就是竟可能的提高短语的word vector和对应filter的vector之间的cos相似性，从而一个filter，只善于获取对应的这个短语。综上pooling layer的功能就是，获取最满足对应filter特征的词向量，并把和其内积的结果传送到下一层网络。<br>\n",
    "上文中，同时也指出了，一个filter，只可以提取一个特征，因此，当我们需要特多的特征的时候，我们可以：use multiple filter weights w。so we can have some filters that look at unigrams, bigrams, trigram, 4-grams, etc.我们不用为操心，为什么相同长度的filter，会获取不同的feature，除非他们的初始化的值是一样的。<br>\n",
    "另外，filter的初始化很重要，至少你应该保证大部分的内积都是大于零的，否则对应LeRU神经元就可能挂了。\n",
    "\n",
    "#### Multi-channel idea\n",
    "- Initialize with pre-trained word vectors(word2Vec or Glove)\n",
    "- start with two copies\n",
    "- Backprop into only one set, keep other \"static\"\n",
    "- Both channels are added to $c_i$ before max-pooling\n",
    "因为当进行训练的时候，原词向量会进行改变，使更适合训练目标。word vectors are really great. we can train them on a very large unsupervised scope so they capture semantic similarities. Now if you start backprogagating your specific task into the wrod vectors,they will start to move around.这引起的问题是，仅存在在训练集中的词向量，会被调整，而那些不存在在训练集中的，但却出现在测试集中的词向量，他还保持着他原来的样子，从而可能会被错误的分类（因为filter vector是针对新的word vector的）。通过增加一个channel，我们在获得更适合当前任务的word vector的同时，保留了原来的，善于表达语义的词向量。（不能体会双channel的好处，因为最终都是要进入max pool的，test的时候就算加入原词向量，想来也是不会有影响的。所以究竟有什么用呢？？除非针对相同的feature，每个channel各自有一个属于自己的filter，而不是使用共同的filter）\n",
    "\n",
    "#### classification after one CNN layer\n",
    "使用softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这期的highlight是：Character-aware Neural Language Models\n",
    "Techniclal Approach:<br>\n",
    "Character Embeddings -> CNN -> Highway Network -> LSTM -> Prediction<br>\n",
    "Motivation:<br.\n",
    "- Derive a powerful, robust language model effective across a variety of language\n",
    "- Encode subword relatedness:eventful, eventfully, uneventful...\n",
    "- Address rare-word problem of prior models\n",
    "- Obtain comparable expressivity with fewer parameters\n",
    "没看懂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
