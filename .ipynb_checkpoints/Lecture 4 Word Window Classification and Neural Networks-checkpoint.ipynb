{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内容概要：\n",
    "1. 分类的基础知识\n",
    "2. updating word vectors for classification \n",
    "3. window classification & cross entropy error derivation tips\n",
    "4. 单层神经元网络\n",
    "5. max margin loss and backprop\n",
    "\n",
    "这节的内容，大部分都是学过的，就不详细说了，只提一下，其中比较需要注意的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需要注意的地方：\n",
    "1. loss function中使用正则项，是为了使各个参数，尽可能的接近于零（注意平方），原话是：<br>\n",
    "This part of the objective function(regularrization term) will try to encourage the model to keep all the weights as small as possible and as close as possible to zero.<br>\n",
    "You can kind of assume if you want as a Bayesian that you can have a prior, a Gaussian distributed prior that says ideally all these are small numbers?? Often times if you don't have this regularization term your numbers will blow up and it will start to overfit more and more.<br>\n",
    "奥卡姆剃刀Occam's razor原理还是可以理解的，也就是：在所有可能选择的模型中，能够很好的解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。<br>\n",
    "但是不理解，为什么说从贝叶斯的角度，复杂的模型有较大的先验概率。但是最小化w，能够降低模型复杂度倒是可以理解的。首先应该理解为什么用复杂的模型拟合简单的例子容易过拟合。举个例子，用五元方程拟合直线上的五个点，虽然他能够“完美”拟合，但仅在对应的点上能够正确进行预测，但对于其他的点的预测确存在问题，这也就是我们说的过拟合。现在假设我们的模型框架就是这个五元方程，参数w对应的是各个元的系数，那么我们应该怎么防止这个模型过拟合呢？其答案就是，尽量减小参数w的大小（使用第二范式使趋近于零，对于第一范式和第二范式的区别请自己在坐标系上画图）使得部分元的系数为零，或接近于零，降低他对整体的影响。我们可以想象到（虽然真的跑的时候，很可能不会出现这种结果），当我们的所有参数中四个参数的大小为零时，我们的模型实际上就是一个直线模型，他能够很好的拟合数据，并且不会有过拟合。当然你也许会说，如果我的实际模型就是五元方程模型呢，对此我只能说，奥卡姆剃刀只是一种偏好，他符合大多数的情况，但并没有直接的证明可以说明越简单的就一定是越好的。\n",
    "2. updating word vectors for classification是指，随机初始化word vector，通过训练，不断调整word vector的值，使得同一个分类的words，的word vector能够聚集在一起。（这种方法对训练集的要求比较大）另外在中window classification的使用只更新center vector，而不是整个window。<br>\n",
    "我们不经常做单个单词的分类，因为针对不同的context，同一个单词可能存在多个意思。所以我们比较常做的是window classification。即根据center word及其window范围内的所有内容（比如，单个单词vector长为d， 单侧window长为2，那么现在单个输入就是5d*1），对center wrod进行分类。<br>\n",
    "3. 教程里刚开始用的是简单的softmax classifier。但是softmax only gives linear decision boundaries in the original space. With little data that can be a good regularizer. with more data it is a very limiting。softmax classifier就是简单的把输入和W进行点乘，然后带入softmax函数。可以把输入当成空间上的点，W当成超平面的方向量，点乘即是点到平面的距离，softmax使用1对n的分类方法（就只有一个类为真，其他为假，因此如果有5个类，他会为每个类创建一个w），他的训练最终使得，对应类中的点在超平面的正方向上尽可能的远离（注意softmax中的指数函数）。放在二维空间上超平面就是一条直线，所以说是linear decision boundaries（关键原因不是在softmax上，而是在没有激活函数上）\n",
    "4. matrix运算的效率要好很多\n",
    "5. max-margen objective function: <br>\n",
    "<center>$J\\ =\\ max(0,\\ 1\\ -\\ s\\ +\\ s_c)$</center>\n",
    "其中s代表正例的得分，$s_c$代表负例的得分。但是感觉max没有意义，因为前面用了sigmal那么值域就是0~1（这部分用的网络是单层神经元网络，使用sigmoid作为激活函数，用max-margin作为loss function），所以1-s+$s_c$一定是大于0的。另外因为是要最小化loss，所以在这个右项之中，s是越大越好，s_c是越小越好（s对应的是正例s_c对应的是负例）。s和距离成正比例关系，所以也就是正例的距离越大越好。负例反向距离越大越好。<br>\n",
    "$\\bf{题外加三星，当（负-正）例得分相差大于阈值的时候，J直接就等于0，这就避免了追踪多余的已经能够很好分类的例子。}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
