{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内容概要\n",
    "- 复习上一节词向量的内容\n",
    "- what does word2vec really catpture?\n",
    "- how could we capture this essence more effectively?\n",
    "- how can we analyze word vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量补充\n",
    "1. 基本所有在这个课程里面，用到的loss function都不是convex的。所有他的初始化，很重要。之后，会讲解一些方法来避免陷入局部最优的困境。<br>\n",
    "但是在实际操作中，当初始的随机数比较小的时候，上面这个问题的影响并不大。\n",
    "2. 由上一节提到的更新部分，我们可以知道更新矩阵是一个稀疏矩阵。\n",
    "3. 从上节提到的p(o|c)，我们可以看到，每次更新，我们都要跑好一遍词汇表(看p的分母)，这限制了效率的提高。<br>\n",
    "另外的，我们知道很多单词并不会同时出现在一个句子中，这种词与词之间的关系，在整个词汇表中是非常稀疏的，就更别说在一个window之内了。因此我们想到的一个办法就是：<br>\n",
    "我们保留分子部分不变，但我们不再在分母中遍历整个词汇表，而是随机选取一些词汇，并假定他们不会出现在窗口中。（paper中的算法排除了出现window中词汇的可能性，但实际上这种情况是很少见的，因此影响并不大）。改进后的objective function长下面这个样子：\n",
    "<center>$J_t(\\theta)\\ =\\ log(\\ \\sigma(u_o^Tv_c)\\ +\\ \\sum_{i=1}^k\\mathbb{E}_{j \\backsim P(w)}[log\\ \\sigma(-u_j^Tv_c)]$</center>\n",
    "这里我们用了sigmoid方程，而不是上一节用的指数方程。他同样也能消除负数，而他的另一个好处就是，他能把输入值投影到零和一之间，因此我们可以把它当成概率看待。从而loss function的第一部分就可以看出，最大化两个单词同时出现在窗口的中概率的log值。<br>\n",
    "第二部分中，$-\\sigma\\ =\\ 1\\ -\\ \\sigma$，所以我们是最大化，随机子集中各个单词和目标单词不同时出现的概率的log值<br>\n",
    "我们sample的方法是：$P(w) = U(w)^{3/4}\\ /\\ Z$<br>\n",
    "其中U(w)是unigram，幂函数是为了提高低频词汇的出现概率（0~1 => 0~1 幂值越接近1输出越缓）<br>\n",
    "上面这种做法，我们称为negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec究竟是实现了什么，怎么进行优化\n",
    "他实现了把语法上，甚至词法上相近的词进行聚类。\n",
    "这个算法的核心是，如何获取两个单词一起出现的概率。<br>\n",
    "打个比方我们处理deep和learning这两个词，我们在window中进行观察，每出现一次，我们给他记一次数，并更新他们的词向量。然后移动window，进行相同的操作，知道遍历完整个文集。我们会发现这种方法效率并不高，我们为何不遍历完整个文集，记录下这两个词同时出现的次数，最后在进行更新呢？？事实上这也就是我们接下来要提到的方法：<br>\n",
    "最简单的方法就是，我们还继续用上面的方法，用window缓慢的移动遍历全文，但这回我们并做任何的词向量更新，我们只是单纯的进行计数，计算两个单词同时出现的次数（用v*v的矩阵记录，出现一次，对应的数值加一），最后再对这个matrix进行处理。这种方法使我们再关注语义的同时，也不会忽略了句法结构。<br>\n",
    "另外一种方法，我们可以忽略window，而是观察整个文集。这种方法我们获得的，更多的是关于这篇文章的主题的信息。一般，我们称这种方法为latent semantic analysis。相对的这会导致一些句法信息的遗失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### window based co-occurrence matrix as wordvector\n",
    "problem:<br>\n",
    "1. 每当有新词汇出现，他需要调整vector的大小\n",
    "2. 维度过高，这里等于v的大小。\n",
    "3. you will have sparsity issues if you try to train a machine learning model. 使得生成的模型的robust性比较差\n",
    "\n",
    "优化方法：\n",
    "1. 降维，只记载相对重要的信息，而忽略其他的信息。其中一个方法就是使用SVD\n",
    "2. 因为一些常用词像the，a，他们的匹配实际上并没有什么意义，所以我们可以给计数设置上限，比如100或干脆就忽略前n个最频繁的词组\n",
    "3. 另外一种小方法就是，加权计数，权值由两个单词相距的距离决定，离得越远，对应的权值就越小\n",
    "4. 也可以干脆就放弃计数，而改用从relation。\n",
    "\n",
    "SVD方法的确定\n",
    "1. O($mn^2$)\n",
    "2. 新词困扰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较两类方法的有缺点\n",
    "![count based vs direct prediction](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/count_based_vs_direct_prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe model：上面两货的结合\n",
    "<center>$J(\\theta)\\ =\\ \\frac{1}{2}\\sum_{i,j=1}{W}\\ f(P_{ij})(u_i^Tv_j\\ -\\ log\\ P_{ij})^2$</center>\n",
    "W是所有同时出现过的单词对。P是上面提过的co-occur matrix。记得点乘代表的是相似性，P代表的是一起出现的次数，最小化两者的差的平方（平方表示距离），使得两者存在对应关系。也就是同时出现次数越多的单词，相似性越高。f是用来降低过于频繁的词组，像带the出门的那些啊，的影响。记得前面用的是幂函数，也可以使其他的。<br>\n",
    "\n",
    "这种方法的好处是：<br>\n",
    "1. 训练很快\n",
    "2. 伸展性好scalability\n",
    "3. 即使是对小的文集，表现得也不差\n",
    "\n",
    "记得我们用的方法中每个单词都用两个vector进行表示，那么我们要怎么合并他们呢：<br>\n",
    "最简单的，也是最好的，方法就是直接相加。<br>\n",
    "当然，你也可以直接把相同的词链接在一起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate word vectors\n",
    "![evaluate word vectors](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/evaluate_word_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intrinsic word vector evaluation:\n",
    "Word Vector Analogies:<br>\n",
    "比如已知man对应woman，那么King会对应什么？我们用a代表man，b代表woman，c代表king，那么对应的词我们可以通过以下方法求得：\n",
    "<center>$d\\ =\\ argmax_i\\ \\frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||}$</center>\n",
    "也就是去cos similarity最大的那个家伙。为什么方程长这样子，请自行画坐标图。\n",
    "一个好的word vector应该输出queen。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面Evaluation部分没有记录，并不是因为它不重要。事实上，他太重要了。没有记录的唯一原因是，他展示的例子并不是重点，重点是知道怎么Evaluation及其原理，虽然这些例子，真的对你理解这个过程很有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
