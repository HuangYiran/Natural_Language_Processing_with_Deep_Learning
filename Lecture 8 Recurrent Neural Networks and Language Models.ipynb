{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### language Models\n",
    "a language model computes a probability for a sequence of words $P(w_1, w_2, ..., w_T)$<br>\n",
    "动机之一可以是以此作为机器翻译中候选的选择标准。<br>\n",
    "不同语言的语序也会不一样，比如英语中the cat is small的概率就应该比small the is cat的大。这估计是机器翻译要用编码-解码机制的原因吧。\n",
    "#### Traditional Language Models\n",
    "Probability is usually conditioned on window of n previous words<br>\n",
    "an incorrect but necessary Markov assumption!<br>\n",
    "$p(w_1,...,w_m)\\ =\\ \\prod_{i=1}^mP(w_i|w_1,...,w_{i-1})\\ \\approx\\ \\prod_{i=1}^mP(w_i|w_{i-(n-1)},...,w_{i-1})$<br>\n",
    "等式成立原因，具体应该看贝叶斯网路。另外根据贝叶斯公式计算每个因子（条件概率）的值。<br>\n",
    "约等号后面表示的是ngram模型。即有前n个字推出下一个字。这些数据越完备，最后计算效率越高<br>\n",
    "Performance improves with keeping around higher n-grams counts and doing smoothing and so-called backoff(e.g. if 4-gram not found, try 3-gram, etc)<br>\n",
    "这种方法的缺点是内存需求量巨大。想想10000个单词，可以形成多少个4-gram.当然具体应用中n-gram是由具体文章生成的，这样可以减免很多不必要的组合，但尽管如此，数量还是巨大。google n-gram项目中一种语言的2-gram大小即可达到十几G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "因为是老姿势，挑有趣的加上基础的写写：<br>\n",
    "at a single time step: <br>\n",
    "<center>$h_t\\ =\\ \\sigma(W^{(hh)}h_{t-1}\\ +\\ W^{(hx)}x_{[t]})\\\\ \\hat{y}_t\\ =\\ softmax(W^{S}h_t)$</center>\n",
    "h的维度和隐藏层的输出维度一样，都是[h, 1]。隐藏层接h的W的维度是[h, h]。隐藏层接输入的W的维度是[h, x]。<br>\n",
    "最后一层输出使用softmax，也就是说参数W的维度是[|V|, h]，V表示词汇量。可以想象这个matrix的大小<br>\n",
    "object function可以用cross entropy:<br>\n",
    "<center>$J^{(t)}(\\theta)=-\\sum_{j=1}^{|V|}y_{t,j}log{\\hat{y}_{t,j}}$</center>\n",
    "但更经常的，我们使用perplexity：\n",
    "Evaluation could just be negative of average log probability over dataset of size (number of words) T:\n",
    "<center>$J\\ =\\ -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{j=1}^{|V|}y_{t,j}log\\hat{y}_{t,j}$</center>\n",
    "Perplexity = $2^J//$\n",
    "lower is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练讲到梯度消失（改变太弱或者太强?）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
