{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### language Models\n",
    "a language model computes a probability for a sequence of words $P(w_1, w_2, ..., w_T)$<br>\n",
    "动机之一可以是以此作为机器翻译中候选的选择标准。<br>\n",
    "不同语言的语序也会不一样，比如英语中the cat is small的概率就应该比small the is cat的大。这估计是机器翻译要用编码-解码机制的原因吧。\n",
    "#### Traditional Language Models\n",
    "Probability is usually conditioned on window of n previous words<br>\n",
    "an incorrect but necessary Markov assumption!<br>\n",
    "$p(w_1,...,w_m)\\ =\\ \\prod_{i=1}^mP(w_i|w_1,...,w_{i-1})\\ \\approx\\ \\prod_{i=1}^mP(w_i|w_{i-(n-1)},...,w_{i-1})$<br>\n",
    "等式成立原因，具体应该看贝叶斯网路。另外根据贝叶斯公式计算每个因子（条件概率）的值。<br>\n",
    "约等号后面表示的是ngram模型。即有前n个字推出下一个字。这些数据越完备，最后计算效率越高<br>\n",
    "Performance improves with keeping around higher n-grams counts and doing smoothing and so-called backoff(e.g. if 4-gram not found, try 3-gram, etc)<br>\n",
    "这种方法的缺点是内存需求量巨大。想想10000个单词，可以形成多少个4-gram.当然具体应用中n-gram是由具体文章生成的，这样可以减免很多不必要的组合，但尽管如此，数量还是巨大。google n-gram项目中一种语言的2-gram大小即可达到十几G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "因为是老姿势，挑有趣的加上基础的写写：<br>\n",
    "at a single time step: <br>\n",
    "<center>$h_t\\ =\\ \\sigma(W^{(hh)}h_{t-1}\\ +\\ W^{(hx)}x_{[t]})\\\\ \\hat{y}_t\\ =\\ softmax(W^{S}h_t)$</center>\n",
    "h的维度和隐藏层的输出维度一样，都是[h, 1]。隐藏层接h的W的维度是[h, h]。隐藏层接输入的W的维度是[h, x]。<br>\n",
    "最后一层输出使用softmax，也就是说参数W的维度是[|V|, h]，V表示词汇量。可以想象这个matrix的大小<br>\n",
    "object function可以用cross entropy:<br>\n",
    "<center>$J^{(t)}(\\theta)=-\\sum_{j=1}^{|V|}y_{t,j}log{\\hat{y}_{t,j}}$</center>\n",
    "但更经常的，我们使用perplexity：\n",
    "Evaluation could just be negative of average log probability over dataset of size (number of words) T:\n",
    "<center>$J\\ =\\ -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{j=1}^{|V|}y_{t,j}log\\hat{y}_{t,j}$</center>\n",
    "Perplexity = $2^J$<br>\n",
    "lower is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度消失（改变太弱或者太强）\n",
    "当输入和输出都是向量的时候，输出对输入的偏导就是一个Jacobian\n",
    "<center>$\\frac{\\delta E_t}{\\delta W}\\ =\\ \\sum_{k=1}^{t}\\frac{\\delta E_t}{\\delta y_t}\\frac{\\delta y_t}{\\delta h_t}\\frac{\\delta h_t}{\\delta h_k}\\frac{\\delta h_k}{\\delta W}$</center>\n",
    "由于$h_t$是$h_{t-1}$的函数，因此有：\n",
    "<center>$\\frac{\\delta h_t}{\\delta h_k}\\ =\\ \\prod_{j=k+1}^{t}\\frac{\\delta h_j}{\\delta h_{j-1}}$</center>\n",
    "而上面的每个偏导都是Jacobian，那是相当的大啊，因此我们应该想办法简化他们：\n",
    "<center>$||\\frac{\\delta h_j}{\\delta h_{j-1}}||\\ \\leq \\ ||w^T||\\ ||diag[f'(h_{j-1})]||\\ \\leq \\ \\beta_W\\beta_h$</center>\n",
    "从后向传播可以得到第一个小于号，$\\frac{\\delta s}{\\delta x}\\ =\\ W^T\\delta$，这里是取模，所以是小于或等于。$\\beta_W\\beta_h$只是单纯的表示上界。<br>\n",
    "因此有：\n",
    "<center>$||\\frac{\\delta h_t}{\\delta h_k}||\\ =\\ ||\\prod_{j=k+1}^{t}\\frac{\\delta h_j}{\\delta h_{j-1}}||\\ \\leq\\ (\\beta_W\\beta_h)^{t-k}$</center>\n",
    "This can become very small or very large quickly, and the locality assumption of gradient descent breaks down -> Vanishing or exploding gradient<br>\n",
    "这个vanishing gradient和一般的解释不大一样，这里是针对RNN这种情况的，而其他的则是偏向具体的激活函数。但是都挺有道理的不是吗。\n",
    "我们想要的是：The error at a time step ideally can tell a previous time step from many steps away to change during backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick for exploding gradient: clipping trick\n",
    "The solution first introduced by Mikolov is to clip gradients to a maximum value.<br>\n",
    "就是当gradient超过一个阈值的时候，就把他给剪了。实践证明效果还是相当不错的。<br>\n",
    "但是与之相反的，vanish gradient，你不能直接给他乘以一个数，因为这会导致overshot.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vanish_gradient](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/vanish_gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个一元RNN的训练图。可以看见图中存在一个曲面，而局部最低，就在这个曲面旁边。随着训练参数会逐渐向曲面靠近（实际是在靠近局部最小）。问题是只要一碰到曲面，参数就会被直接弹得飞远。这里clip的好处就体现出来了，它能够让碰到曲面的时候，不至于被弹飞，从而迷路，回不来。反之在vanishing gradient中直接扩大的方法就相当于，还没碰到曲面，就自己飞起，然后就不知所踪了。就相当于人为的加重了，久远历史的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介绍了一个测量curvature的方法，其实我还没有体会到curvature的重要性，呵呵哒。<br>\n",
    "背景是，在实际操作种，训练的维度一般都是非常大的，从而导致难以进行可视化，进而无法知道训练的各个阶段的curvature的大小。<br>\n",
    "他提出的方法是，用一条线连接训练前后的参数。用特定大小的窗口顺序在这条直线上取样（移动距离为窗口大小），计算每个窗口中面积（实际曲线和所做直线之间形成的面的面积）的大小，比较前后两个窗口，面积相差越大，说明这个地方的curvature越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For vanishing gradients: Initialization + ReLus\n",
    "Initialize $W^{(*)}$'s to identity matrix I and f(z) = rect(z) = max(z, 0)<br>\n",
    "初始化的思想是：一般的我们的W是随机初始化的，也就是说，他把对应的输入随机的导向一个方向，而我们并不知道这个方向是指向哪里的。当我们初始化为identity的时候，他保持了初始输入。（但是并不理解，为什么这样就能够得到优化，想了解具体情况的就是看paper吧。但是这个配合上ReLU确实可以使指数跳跃的不那么过分。至少比sigmoid是要好些的。 实际效果惊人！！！吓呆了）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem： Softmax is huge and slow\n",
    "就是前面提到的大矩阵问题。\n",
    "Trick: Class-based word prediction<br>\n",
    "$P(W_T|history)\\ =\\ p(c_t|history)\\ p(w_t|c_t)\\ =\\ p(c_t|h_t)\\ p(w_t|c_t)$<br>\n",
    "The more classes, the better perplexity but also worse speed.<br>\n",
    "就是给词汇分类，用history来推断目标单词是落在那个分类里面，再根据该单词在对应类中的频率，推导出对应的单词。有点像optimized贝叶斯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One last implementation trick\n",
    "You only need to pass backwards through your sequence once and accumulate all the deltas from each $E_t$\n",
    "详见BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional RNNs && Deep Bidirectional RNNs\n",
    "O(∩_∩)O~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "7 7\n",
      "8 8\n",
      "9 9\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(range(10)):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
