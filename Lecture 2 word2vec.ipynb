{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 内容概要\n",
    "- 词意\n",
    "- 何为Word2vec\n",
    "- 聚光灯\n",
    "- word2vec中objective function的梯度计算\n",
    "- optimization refresher\n",
    "- word2vec的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何表达词意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is meaning:\n",
    "1. 由单词表达出来的意思\n",
    "2. 人类通过单词想表达的意思\n",
    "\\# 注意上面两货的区别\n",
    "<br>一般在语言学上：signifoer <=> signified(idea or thing) = denotation\n",
    "\n",
    "通过nltk包，可以获得一个单词相关的词意信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synset('panda.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这种获得语义的缺点是：\n",
    "1. 好但并不全面\n",
    "2. missing new word\n",
    "3. subjective\n",
    "4. 需要人为创建和调整\n",
    "5. 难以计算单词的相似性，及衡量这个计算的精确性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最古老的词向量表示方法：one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个单词由一个1和一堆0表示，所有词向量等长，长度大小由词汇表决定<br>\n",
    "one-hot的最大的缺点并不是他的肥胖，而是他没有办法表达词汇之间的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distributional similarity\n",
    "核心思想是：一个单词的意义，可以通过他的上下文来表达：<br>\n",
    "一个经典的例子是：<br>\n",
    "government debt problems turning into banking crises as has happend in <br>\n",
    "&emsp;&emsp;&emsp;saying that Europe needs unified banking regulation to replace the hodgepodge<br>\n",
    "在这个例子中，两个banking的意思都可以通过他的上下文来表达"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们的做法是，为每个单词寻找对应的词向量，使得这个词向量可以很好的预测，该单词上下文中的其他单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型，使给定句子及其中心词，可以得到对应的： $p(context|\\ w_t)$ <br>\n",
    "定义loss方程：$J = 1 - p(w_{-t}|\\ w_t$)。通过调整词向量，使loss达到最优化<br>\n",
    "\\# 负号在这里表示除此以为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而deep learning的神奇之处就在于，你只需要有这么一个目标，你不必知道如何去实现他，然后dnn就可以帮你把它完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回到Word2vec上\n",
    "word2vec就是predict between every word and its context words!\n",
    "一般有两个算法思路：\n",
    "- skip-grams: word -> context\n",
    "- Continuous Bag of Words(CBOW): context -> word\n",
    "\n",
    "两种基本的训练方法：\n",
    "- hierarchical softmax\n",
    "- Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram\n",
    "核心思想是，给定一句话和窗口大小，对于这句话中的每个单词，预测在其窗口范围内对应其他单词出现的概率$p(w_{t-i}|\\ w_t)$。然后通过调参，优化这些概率<br>\n",
    "这里的$\\theta$表示词向量！！\n",
    "<center>$J'(\\theta)\\ = \\ \\prod_{t = 1}^T\\prod_{-m<=j<=m,j\\not=0}p(w_{t + j}|\\ w_t;\\theta)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面那货是我们想要实现的，但在实际中我们并不直接的使用它，而是对它做一些变形，在这里\n",
    "我们使用negetive log likelihood:<br>\n",
    "这里的$\\theta$表示用于调整的变量\n",
    "<center>$J(\\theta)\\ = \\ -\\frac{1}{T}\\sum_{t = 1}^T\\sum_{-m<=j<=m,j\\not=0}log\\ p(w_{t+j}|w_t)$</center>\n",
    "$\\frac{1}{T}$用于取均值，‘-’用于使求最大值，转而为求最小值<br>\n",
    "\n",
    "在这里一下名词是等价的:<br>\n",
    "Loss function = cost function = objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么如何确定$p(w_{t+j}|\\ w_t)$，在这里我们使用softmax，softmax的输入是，目标单词词向量和其邻域各个单词的词向量的点乘。<br>\n",
    "<center>$p(o|c) = \\frac{exp(u_o^T\\ v_c)}{\\sum_{w=1}^vexp(u_w^T\\ v_c)}$</center>\n",
    "在这里u和v都是词向量列表，u对应的是context中的单词，而v对应的是目标单词。o和c是其对应的下标<br>\n",
    "\\# 在这里我们也可以看出，一个单词并不是只能由一个词向量，所以v在这里是词向量列表而不是单个词向量<br>\n",
    "\\# 一般每个单词用两个词向量来表达，一个是它作为center word的时候用的，另一个是它作为context word时用的\n",
    "\n",
    "点乘是对相识性的一种模拟，我们的想法是使用点乘的大小来表示两个词之间的相识性：乘积越大，对应的相似性也就是越大（再次的，我们只是给出想法，具体实现，就不用管了）<br>\n",
    "softmax就不解释了，总之就是模拟概率分布的：把数据转化成对应的概率分布<br>\n",
    "<center>$p_i\\ =\\ \\frac{e^{u_i}}{\\sum_je^{u_j}}$</center>\n",
    "使用指数函数，是为了使目标数据转换成正值。原文是:<br>\n",
    "if you exponentiate things that puts them into positive land <br>\n",
    "但是不是改变了数值之间的间距了吗，负数将被压缩，而正值将被放大？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skip_gram](https://raw.githubusercontent.com/HuangYiran/Natural_Language_Processing_with_Deep_Learning/master/skip-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "教授这里用的是每个单词由两个词向量的版本：一个作为context word一个作为center word<br>\n",
    "所以这里说W表示的是center word的词向量集是正确的。\n",
    "而对应的，这里的parameter也就是原来的两倍$\\in\\ R^{2dV}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算梯度\n",
    "这个比较简单，就直接po结果了：关键是用chain rule\n",
    "<center>$\\delta J(\\theta)\\ =\\ u_o + \\sum_{x = 1}^V\\ p(x|\\ c) *\\ u_x$</center>\n",
    "<br>\n",
    "我们可以看出这个值由两部分组成：前一部分是obserd后一部分是输出的期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用dot代表相似性的原因参考cos similarity。但它并不是必须的，也可以用别的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新\n",
    "最一般的，乘以leraning rate然后直接减就是了<br>\n",
    "<center>$\\theta^{new}\\ =\\ \\theta^{old}-\\ \\alpha\\nabla_\\theta J_t(\\theta)$</center>\n",
    "梯度下降有时候表现得不咋滴，一种原因是因为跨步太大了，导致了越过最小值，这时候就应该保证learning rate足够小，来确保能够缓步走向最优。<br>\n",
    "针对word2vec还存在另一个问题，就是词汇量太大，如果每步都要更新所有的单词，那么，他能够很快的把内存给炸掉，而且时间上想必也是不允许的。所以上面这种方法就是说着玩的，基本没人会用。<br>\n",
    "针对这个问题，我们一般使用SGD,Stochastic gradient descent:<br>\n",
    "我们只对目标单词和其窗口范围内的单词进行调整，其他单词的梯度都置零。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "下面，这一段不是很理解：<br>\n",
    "This estimate of the gradient is incredibly, incredibly noisy, because we've done it at one position which just happens to have a few words around it. So the vast mojority of the parameters of our model, we didn't see at all. So, it's a kind of incredibly noisy estimate of the gradient. walking a little bit in tha direction isn't even guaranteed to have make you walk downhill, because it's such a noisy estimate. But in practice, this works like a gem. And in fact, it works better. Again, it's a win, win. It's not only that doing things this way is orders of magnitude faster than batch gradient descent, because you can do an update after you look at every center word position. It turns out that neural network algorithms love noise. So the fact that this gradient descent, the estimate of the gradient is noisy, actually helps SGD to work better as an optimization algorithm and neural network learning.<br>\n",
    "这里的noisy应该怎么理解？？？<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "highlight那部分跳了，不知道为什么插入那个<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
